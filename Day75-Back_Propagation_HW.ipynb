{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業目標:\n",
    "\n",
    "    通過建立多層的神經網路, 了解權值矩陣更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業重點: \n",
    "\n",
    "3  層神經網路\n",
    "\n",
    "通過增加更多的中間層，以對更多關係的組合進行建模\n",
    "\n",
    "syn1 權值矩陣將隱層的組合輸出映射到最終結果，\n",
    "\n",
    "而在更新 syn1 的同時，還需要更新 syn0 權值矩陣，\n",
    "\n",
    "以從輸入資料中更好地產生這些組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "# Sigmoid 函數可以將任何值都映射到一個位於 0 到  1 範圍內的值。通過它，我們可以將實數轉化為概率值\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])  \n",
    "        \n",
    "# define y for output dataset            \n",
    "op = np.array([3,5,9])\n",
    "y= np.dot(X,op)\n",
    "y=y.reshape(4,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 9] (3,)\n",
      "[[ 9]\n",
      " [14]\n",
      " [12]\n",
      " [17]] (4, 1)\n"
     ]
    }
   ],
   "source": [
    "##check y\n",
    "print(op,op.shape)\n",
    "print(y,y.shape)\n",
    "#op=np.array([[3],[5],[9]])\n",
    "#Y=X*op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "#亂數設定產生種子得到的權重初始化集仍是隨機分佈的，\n",
    "#但每次開始訓練時，得到的權重初始集分佈都是完全一致的。\n",
    " \n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "# define syn1\n",
    "\n",
    "iter = 0\n",
    "#該神經網路權重矩陣的初始化操作。\n",
    "#用 “syn0” 來代指 (即“輸入層-第一層隱層”間權重矩陣）\n",
    "#用 “syn1” 來代指 (即“輸入層-第二層隱層”間權重矩陣）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add \n",
    "syn1 = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n"
     ]
    }
   ],
   "source": [
    "##check \n",
    "print(syn0.shape)\n",
    "print(syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神經網路訓練\n",
    "for 迴圈反覆運算式地多次執行訓練代碼，使得我們的網路能更好地擬合訓練集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f303d224dcf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ml0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(\"l0\",l0.shape, \"syn0\",syn0.shape,\"y\",y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    #print(\"l0\",l0.shape, \"syn0\",syn0.shape,\"y\",y.shape)\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    \n",
    "    '''\n",
    "    新增\n",
    "    l2_error 該值說明了神經網路預測時“丟失”的數目。\n",
    "    l2_delta 該值為經確信度加權後的神經網路的誤差，除了確信誤差很小時，它近似等於預測誤差。\n",
    "    '''\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    " \n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "    #print(\"shape:\",\"l1_delta\",l1_delta.shape, \"l1_error\",l1_error.shape)\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "    #print(syn0)\n",
    "     # syn1 update weights\n",
    "    \n",
    "print(\"Output After Training:\")\n",
    "print(l1)\n",
    "print(\"\\n\\n\")\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99977125],\n",
       "       [-0.55912226],\n",
       "       [-1.16572724],\n",
       "       [-0.72507825]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(l0,syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "te= nonlin(np.dot(l0,syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape,te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_error = y - l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.7310136 , 13.7310136 , 11.7310136 , 16.7310136 ],\n",
       "       [ 8.63624942, 13.63624942, 11.63624942, 16.63624942],\n",
       "       [ 8.76237183, 13.76237183, 11.76237183, 16.76237183],\n",
       "       [ 8.6737243 , 13.6737243 , 11.6737243 , 16.6737243 ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2_Error:0.49641003190272537\n",
      "L1_Error:0.49641003190272537\n",
      "L2_Error:0.008584525653247152\n",
      "L1_Error:0.008584525653247152\n",
      "L2_Error:0.005789459862507806\n",
      "L1_Error:0.005789459862507806\n",
      "L2_Error:0.004629176776769986\n",
      "L1_Error:0.004629176776769986\n",
      "L2_Error:0.003958765280273646\n",
      "L1_Error:0.003958765280273646\n",
      "L2_Error:0.0035101225678616753\n",
      "L1_Error:0.0035101225678616753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    " \n",
    "'''\n",
    "定義數學函數:\n",
    "y=f(x) = 1/(1+np.exp(-x));\n",
    "dy/dx = df = x*(1-x) \n",
    "\n",
    "為了計算方便, 這邊把 y=f(x) 與 dy/dx 放在同一function 裡面;\n",
    "利用 deriv (derivative)做變數, 來分別指定方程式\n",
    "\n",
    "'''    \n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    " \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# 參數定義\n",
    "# X 輸入資料集，形式為矩陣，每 1 行代表 1 個訓練樣本。\n",
    "# y 輸出資料集，形式為矩陣，每 1 行代表 1 個訓練樣本。\n",
    "\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    " \n",
    "y = np.array([[0],\n",
    "            [1],\n",
    "            [1],\n",
    "            [0]])\n",
    " \n",
    "np.random.seed(1)\n",
    " \n",
    "# randomly initialize our weights with mean 0 \n",
    "# syn0 第一層權值\n",
    "# syn1 第二層權值\n",
    "\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "syn0_history = [syn0]\n",
    "syn1_history = [syn1]\n",
    " \n",
    "for j in range(60000):\n",
    " \n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    # l0 網路第 1 層，即網路輸入層。\n",
    "    # l1 網路第 2 層，常稱作隱藏層。\n",
    "    # l2 假定為網路最後一層，隨著訓練進行，其輸出應該逐漸接近正確結果\n",
    "    \n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    " \n",
    "    # how much did we miss the target value?\n",
    "    # l2_error 該值說明了神經網路預測時“丟失”的數目。\n",
    "    # l2_delta 該值為經確信度加權後的神經網路的誤差，除了確信誤差很小時，它近似等於預測誤差。\n",
    "    # l1_error 該值為 l2_delta 經 syn1 加權後的結果，從而能夠計算得到中間層/隱層的誤差。\n",
    "    # l1_delta 該值為經確信度加權後的神經網路 l1 層的誤差，除了確信誤差很小時，它近似等於 l1_error 。\n",
    "    \n",
    "    \n",
    "    l2_error = y - l2\n",
    " \n",
    "    if (j% 10000) == 0:\n",
    "        print(\"L2_Error:\" + str(np.mean(np.abs(l2_error))))\n",
    " \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    " \n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print(\"L1_Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    " \n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "    syn1_history.append(syn1)\n",
    "    syn0_history.append(syn0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
